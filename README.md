# Comment Toxicity Detection

This project is focused on detecting toxicity in online comments using a deep learning approach. The model classifies comments into different categories of toxicity, such as moderate, high, threatening, and racist. It employs natural language processing (NLP) techniques, and a hybrid architecture of CNN and LSTM to manage word embeddings and text classification. The model is deployed using a Gradio web application, allowing users to input comments and receive real-time toxicity predictions.

## Key Features:

#### 1. Classification
   * **Moderately Toxic Comments**
   * **Highly Toxic Comments**
   * **Threatening Comments**
   * **Racist Comments**

#### 2. Deep Learning Architecture
   * **CNN:** Extracts features from comments.
   * **LSTM:** Captures word dependencies and context.
   * **Dense Layers:** Performs the final classification into toxicity categories.

#### 3. Real-time Detection
   * **Gradio Interface**: Input comments and instantly receive toxicity classification results.

#### 4. Gen Z Term Adaptation
   * **Incorporates Gen Z Language** to ensure accurate detection of modern-day internet slang and terms, improving model efficiency based on feedback.

## Project Structure
â”œâ”€â”€ data/ â”‚ â”œâ”€â”€ train.csv # Training dataset â”‚ â”œâ”€â”€ test.csv # Testing dataset â”œâ”€â”€ models/ â”‚ â”œâ”€â”€ toxicity_model.h5 # Trained model in h5 format â”œâ”€â”€ notebooks/ â”‚ â”œâ”€â”€ data_preprocessing.ipynb # Data cleaning and preprocessing notebook â”‚ â”œâ”€â”€ model_training.ipynb # Model architecture and training notebook â”œâ”€â”€ app/ â”‚ â”œâ”€â”€ gradio_app.py # Gradio web app for model deployment â”œâ”€â”€ README.md # Project description and instructions â””â”€â”€ requirements.txt # Python dependencies


## Installation

### Prerequisites
- Python 3.7+
- TensorFlow 2.x
- Gradio 3.x

### Setup Instructions
## Usage
Once the Gradio web app is running, users can input any comment, and the model will classify it into the following categories:

   * **Moderately Toxic**
   * **Highly Toxic**
   * **Threatening**
   * **Racist**

The output will display a binary label (0 for non-toxic, 1 for toxic) for each category, helping users identify harmful content quickly and effectively.

## Dataset
The dataset used in this project contains a collection of online comments, each labeled with one or more toxicity categories. The comments undergo preprocessing, including:

   * **Tokenization**: Splitting the text into individual words or tokens.
   * **Embedding**: Converting the tokens into numerical vectors for model input.

The dataset is split into training and testing subsets, ensuring the model can generalize well to unseen data.

## Future Improvements
   * **Multilingual Support**: We plan to extend the model to handle multiple languages, allowing for toxicity detection across diverse linguistic contexts.
   * **Sentiment Analysis**: Incorporating sentiment analysis to gauge the overall emotional tone of comments, providing a more comprehensive content analysis.
   * **Additional Toxicity Categories**: Expanding the model to include more specific types of toxicity, such as misogyny or homophobia, to improve detection granularity.

## Developers
   * [Anushya Varshini K](https://github.com/anushya03)
   * [Rithick M K](https://github.com/rithick-06)
   * [Gokulnath G](https://github.com/GOKULNATH004)
ğŸš€ Comment Toxicity Detection

This project focuses on detecting toxicity in online comments using a deep learning approach. The model classifies comments into different toxicity categories, such as moderate, high, threatening, and racist. It employs Natural Language Processing (NLP) techniques and a hybrid CNN-LSTM architecture to manage word embeddings and text classification. The model is deployed using a Gradio web application, allowing users to input comments and receive real-time toxicity predictions.

ğŸ”¥ Key Features

ğŸ·ï¸ Classification Categories

ğŸŸ¡ Moderately Toxic Comments

ğŸ”´ Highly Toxic Comments

âš ï¸ Threatening Comments

ğŸ´ Racist Comments

ğŸ§  Deep Learning Architecture

ğŸ–¼ï¸ CNN: Extracts essential features from comments.

ğŸ“– LSTM: Captures word dependencies and contextual meaning.

ğŸ“Š Dense Layers: Performs final classification into toxicity categories.

âš¡ Real-time Detection

ğŸ›ï¸ Gradio Interface: Users can input comments and instantly receive toxicity classification results.

ğŸ¯ Gen Z Term Adaptation

ğŸ†• Incorporates Modern Internet Slang & Gen Z Language to ensure accurate detection of contemporary online speech patterns, improving model efficiency based on feedback.

ğŸ“‚ Project Structure

â”œâ”€â”€ data/        
â”‚   â”œâ”€â”€ train.csv       # Training dataset
â”‚   â”œâ”€â”€ test.csv        # Testing dataset
â”‚
â”œâ”€â”€ models/       
â”‚   â”œâ”€â”€ toxicity_model.h5 # Trained model in h5 format
â”‚
â”œâ”€â”€ notebooks/  
â”‚   â”œâ”€â”€ data_preprocessing.ipynb # Data cleaning and preprocessing
â”‚   â”œâ”€â”€ model_training.ipynb # Model architecture and training
â”‚
â”œâ”€â”€ app/         
â”‚   â”œâ”€â”€ gradio_app.py # Gradio web app for model deployment
â”‚
â”œâ”€â”€ README.md    # Project documentation
â””â”€â”€ requirements.txt # Python dependencies

ğŸ”§ Installation

ğŸ“Œ Prerequisites

âœ… Python 3.7+

âœ… TensorFlow 2.x

âœ… Gradio 3.x

ğŸ› ï¸ Setup Instructions

# Clone the repository
git clone https://github.com/your-repo/comment-toxicity-detection.git
cd comment-toxicity-detection

# Install dependencies
pip install -r requirements.txt

ğŸš€ Usage

Once the Gradio web app is running, users can input any comment, and the model will classify it into one of the following categories:

ğŸŸ¡ Moderately Toxic

ğŸ”´ Highly Toxic

âš ï¸ Threatening

ğŸ´ Racist

The output will display a binary label (0 for non-toxic, 1 for toxic) for each category, allowing users to quickly identify harmful content.

# Run the Gradio application
python app/gradio_app.py

ğŸ“Š Dataset

The dataset used in this project contains a collection of online comments, each labeled with one or more toxicity categories. The comments undergo preprocessing, including:

âœ‚ Tokenization: Splitting text into individual words/tokens.

ğŸ”¢ Embedding: Converting tokens into numerical vectors for model input.

The dataset is split into training and testing subsets to ensure the model generalizes well to unseen data.

ğŸ”® Future Improvements

ğŸŒ Multilingual Support: Extending the model to handle multiple languages for broader applicability.

ğŸ˜Š Sentiment Analysis: Incorporating sentiment analysis to understand the emotional tone of comments.

ğŸ³ï¸â€ğŸŒˆ Additional Toxicity Categories: Adding detection for misogyny, homophobia, and other nuanced toxicity types.

ğŸ‘¨â€ğŸ’» Developers

ğŸ“ Anushya Varshini K

ğŸ“ Rithick M K

ğŸ“ Gokulnath G

ğŸ’¡ Contributions are welcome! Feel free to open an issue or submit a pull request.

ğŸ“œ License

This project is open-source and available under the MIT License.

