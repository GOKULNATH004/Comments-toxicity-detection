ğŸš€ Comment Toxicity Detection

This project focuses on detecting toxicity in online comments using a deep learning approach. The model classifies comments into different toxicity categories, such as moderate, high, threatening, and racist. It employs Natural Language Processing (NLP) techniques and a hybrid CNN-LSTM architecture to manage word embeddings and text classification. The model is deployed using a Gradio web application, allowing users to input comments and receive real-time toxicity predictions.

ğŸ”¥ Key Features

ğŸ·ï¸ Classification Categories

ğŸŸ¡ Moderately Toxic Comments

ğŸ”´ Highly Toxic Comments

âš ï¸ Threatening Comments

ğŸ´ Racist Comments

ğŸ§  Deep Learning Architecture

ğŸ–¼ï¸ CNN: Extracts essential features from comments.

ğŸ“– LSTM: Captures word dependencies and contextual meaning.

ğŸ“Š Dense Layers: Performs final classification into toxicity categories.

âš¡ Real-time Detection

ğŸ›ï¸ Gradio Interface: Users can input comments and instantly receive toxicity classification results.

ğŸ¯ Gen Z Term Adaptation

ğŸ†• Incorporates Modern Internet Slang & Gen Z Language to ensure accurate detection of contemporary online speech patterns, improving model efficiency based on feedback.

ğŸ“‚ Project Structure

â”œâ”€â”€ data/        
â”‚   â”œâ”€â”€ train.csv       # Training dataset
â”‚   â”œâ”€â”€ test.csv        # Testing dataset
â”‚
â”œâ”€â”€ models/       
â”‚   â”œâ”€â”€ toxicity_model.h5 # Trained model in h5 format
â”‚
â”œâ”€â”€ notebooks/  
â”‚   â”œâ”€â”€ data_preprocessing.ipynb # Data cleaning and preprocessing
â”‚   â”œâ”€â”€ model_training.ipynb # Model architecture and training
â”‚
â”œâ”€â”€ app/         
â”‚   â”œâ”€â”€ gradio_app.py # Gradio web app for model deployment
â”‚
â”œâ”€â”€ README.md    # Project documentation
â””â”€â”€ requirements.txt # Python dependencies

ğŸ”§ Installation

ğŸ“Œ Prerequisites

âœ… Python 3.7+

âœ… TensorFlow 2.x

âœ… Gradio 3.x

ğŸ› ï¸ Setup Instructions

# Clone the repository
git clone https://github.com/your-repo/comment-toxicity-detection.git
cd comment-toxicity-detection

# Install dependencies
pip install -r requirements.txt

ğŸš€ Usage

Once the Gradio web app is running, users can input any comment, and the model will classify it into one of the following categories:

ğŸŸ¡ Moderately Toxic

ğŸ”´ Highly Toxic

âš ï¸ Threatening

ğŸ´ Racist

The output will display a binary label (0 for non-toxic, 1 for toxic) for each category, allowing users to quickly identify harmful content.

# Run the Gradio application
python app/gradio_app.py

ğŸ“Š Dataset

The dataset used in this project contains a collection of online comments, each labeled with one or more toxicity categories. The comments undergo preprocessing, including:

âœ‚ Tokenization: Splitting text into individual words/tokens.

ğŸ”¢ Embedding: Converting tokens into numerical vectors for model input.

The dataset is split into training and testing subsets to ensure the model generalizes well to unseen data.

ğŸ”® Future Improvements

ğŸŒ Multilingual Support: Extending the model to handle multiple languages for broader applicability.

ğŸ˜Š Sentiment Analysis: Incorporating sentiment analysis to understand the emotional tone of comments.

ğŸ³ï¸â€ğŸŒˆ Additional Toxicity Categories: Adding detection for misogyny, homophobia, and other nuanced toxicity types.

ğŸ‘¨â€ğŸ’» Developers

ğŸ“ [Anushya Varshini K](https://github.com/anushya03)

ğŸ“ [Rithick M K](https://github.com/rithick-06)

ğŸ“ [Gokulnath G](https://github.com/GOKULNATH004)

ğŸ’¡ Contributions are welcome! Feel free to open an issue or submit a pull request.

ğŸ“œ License

This project is open-source and available under the MIT License.

